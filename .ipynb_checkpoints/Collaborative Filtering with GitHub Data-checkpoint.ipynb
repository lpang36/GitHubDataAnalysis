{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering #\n",
    "\n",
    "Collaborative filtering is a popular approach to creating recommender systems. Suppose we have the following problem:\n",
    "\n",
    "User A gave five stars to _Titanic_ and _The Notebook_, and one star to _Captain America_ and _Dark Knight Rises_. Should we recommend to User A a film like _Love Actually_? What about _Batman vs Superman_?\n",
    "\n",
    "In this case it's easy to tell what User A prefers since we have prior knowledge about the films. But how can we develop recommendations without this knowledge? \n",
    "\n",
    "It turns out that if we have rating data from many users it's possible to learn the underlying features that define a movie (genre, certain actors/directors, etc.) as well as the preferences of each user for each feature. Then, by identifying the features for a particular movie, and the preferences of a particular user, we can decide whether that movie would be a good recommendation. \n",
    "\n",
    "# Low Rank Matrix Factorization #\n",
    "\n",
    "One method of implementing collaborative filtering is low rank matrix factorization. Suppose we have a matrix of ratings, with one column for each user and one row for each item. Call this matrix $Y$. A small number of the elements in the matrix will have values (the rating user A gives to item B, if it exists), but the vast majority of elements will be unknown. We also have a matrix $R$ which contains 1 in locations where a rating exists and 0 where a rating has not been given. Both $R$ and $Y$ have dimension $n_{items}$ by $n_{users}$. \n",
    "\n",
    "The process described above of learning underlying features and preferences is described mathematically by:\n",
    "\n",
    "$ Y = X \\theta^{T} $\n",
    "\n",
    "$X$ is a matrix with dimension $n_{items}$ by $n_{features}$ and $\\theta$ is a matrix with dimension $n_{users}$ by $n_{features}$. Each row in $X$ is the representation of a particular item as a linear combination of features, and each row in $\\theta$ represents the preferences of a particular user for each feature. Note that a feature might be some characteristic such as genre or author - but it may also be very difficult to interpret. The beauty of the algorithm is that it will automatically choose the most meaningful features.\n",
    "\n",
    "# Training and Implementation #\n",
    "\n",
    "The goal of our algorithm is to find appropriate matrices $X$ and $\\theta$ such that $X \\theta^{T}$ approximates $Y$. However, it only needs to approximate $Y$'s values where a rating has actually been given - for some user A who has not rated item B, the value given by $X \\theta^{T}$ at column A and row B is the _predicted_ rating that user A will give to item B. \n",
    "\n",
    "We can find $X$ and $\\theta$ using gradient descent. The loss function will be the sum-of-squares loss between $X \\theta^{T}$ and $Y$, _but only evaluated on locations where a rating has been given_, i.e. where $R_{i,j}=1$. Similarly, the gradient is only computed at these locations as well. \n",
    "\n",
    "For this particular analysis, the goal is to recommend programming languages for you to learn, given some languages you already like. We will approximate the preferences of users with the languages used in GitHub repositories. The items we rate will be languages. Once we compute $X$ and $\\theta$, the recommended languages will be those with the highest predicted ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Data From BigQuery #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a relatively small number of repositories (for speed) and their language details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository 1\n",
      "C#: 91187 bytes\n",
      "\n",
      "Repository 2\n",
      "Vim script: 8707 bytes\n",
      "\n",
      "Repository 3\n",
      "CSS: 866 bytes\n",
      "JavaScript: 12569 bytes\n",
      "\n",
      "Repository 4\n",
      "Ruby: 3005 bytes\n",
      "\n",
      "Repository 5\n",
      "CSS: 664 bytes\n",
      "Ruby: 1497 bytes\n",
      "\n",
      "Repository 6\n",
      "Ragel in Ruby Host: 4794 bytes\n",
      "Ruby: 61628 bytes\n",
      "\n",
      "Repository 7\n",
      "Batchfile: 1653 bytes\n",
      "CSS: 4059 bytes\n",
      "HTML: 20663 bytes\n",
      "JavaScript: 51872 bytes\n",
      "Ruby: 1509 bytes\n",
      "Shell: 3123 bytes\n",
      "\n",
      "Repository 8\n",
      "\n",
      "Repository 9\n",
      "Ruby: 1926 bytes\n",
      "\n",
      "Repository 10\n",
      "Ruby: 101097 bytes\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "QUERY = \"\"\"\n",
    "        SELECT repo_name, language\n",
    "        FROM `bigquery-public-data.github_repos.languages`\n",
    "        ORDER BY rand()\n",
    "        LIMIT 200\n",
    "        \"\"\"\n",
    "\n",
    "query_job = client.query(QUERY)\n",
    "\n",
    "iterator = query_job.result(timeout=30)\n",
    "rows = list(iterator)\n",
    "\n",
    "for i in range(10):\n",
    "    print 'Repository '+str(i+1)\n",
    "    for j in rows[i].language:\n",
    "        print j[u'name']+': '+str(j[u'bytes'])+' bytes'\n",
    "    print \n",
    "print '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Languages #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all languages in the given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLSL\n",
      "TypeScript\n",
      "Java\n",
      "Scala\n",
      "ApacheConf\n",
      "JavaScript\n",
      "Makefile\n",
      "Perl\n",
      "Lua\n",
      "Erlang\n",
      "...\n",
      "33 languages\n"
     ]
    }
   ],
   "source": [
    "names = {}\n",
    "for i in range(len(rows)):\n",
    "    for j in rows[i].language:\n",
    "        if j[u'name'] in names:\n",
    "            names[j[u'name']]+=1\n",
    "        else:\n",
    "            names[j[u'name']]=1\n",
    "\n",
    "names = [n for n in names if names[n]>1]\n",
    "for i in range(10):\n",
    "    print names[i]\n",
    "print '...'\n",
    "\n",
    "name_to_index = {}\n",
    "for j,i in enumerate(names):\n",
    "    name_to_index[i] = j\n",
    "print str(len(names))+\" languages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repository-Language Matrix #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix where each row represents a repository and each column represents a language. This matrix is our Y (i.e. what we are trying to predict). Here, if a language A is used in repository B, this is considered as repository B giving A a rating. If A is not used in B, then there is no rating (rather than a rating of 0). \n",
    "\n",
    "The value in the matrix is 0 if the language is not present in the repository; otherwise, it is the logarithm of the number of bytes in the repository in that particular language, divided by the sum of the logarithms of the number of bytes of each language in the repository. The logarithm was chosen so the number-of-bytes information could be used, but languages with fewer bytes would not be underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "global mat\n",
    "mat = np.zeros((len(rows),len(names)))\n",
    "for i,row in enumerate(rows):\n",
    "    total = sum([log(lang[u'bytes']) if lang[u'bytes']!=0 else 0 for lang in row[1]])\n",
    "    for lang in rows[i].language:\n",
    "        if lang[u'name'] in name_to_index:\n",
    "            mat[i][name_to_index[lang[u'name']]] = (log(lang[u'bytes']) if lang[u'bytes']!=0 else 0)/total if total!=0 else 0\n",
    "mat = mat[~np.all(mat==0,axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA we can define roughly the number of features we want to identify the low rank matrix factorization. The graph below shows the amount of unexplained variance plotted against the number of components used. The \"elbow\" of the graph (at around n=15) is typically used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe2a2e82d50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2a3435990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "n_components = min(50,len(names))\n",
    "pca = PCA(n_components=n_components)\n",
    "transformed = pca.fit_transform(mat) \n",
    "\n",
    "evr = [1-sum(pca.explained_variance_ratio_[:i+1]) for i in range(len(pca.explained_variance_ratio_))]\n",
    "plt.plot(range(1,n_components+1),evr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Gradient #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some useful functions. \n",
    "\n",
    "init_mask: Create a mask matrix that indicates where Y has meaningful values. \n",
    "\n",
    "loss: Sum-of-squares loss, with a regularization term to prevent overfitting. The matrices theta and X are multiplied to give a \"best guess\", which is then compared with the target matrix Y, but only in locations where a rating has been given.\n",
    "\n",
    "gradient: Derivative of loss with respect to theta and X, with a regularization term. \n",
    "\n",
    "These functions will be useful in performing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 15\n",
    "filter_size = min(100,len(mat[0]))\n",
    "mat = mat[:,range(filter_size)] if len(mat[0])>filter_size else mat #for speed\n",
    "global mask,n_repos,n_langs,reg_param\n",
    "reg_param = 0.00001\n",
    "\n",
    "#mask\n",
    "def init_mask(Y=mat):\n",
    "    f = np.vectorize(lambda x: 1 if x>0 else 0)\n",
    "    return f(Y),len(Y),len(Y[0])\n",
    "\n",
    "#loss\n",
    "def loss(args, Y=mat):\n",
    "    theta = np.reshape(args[:n_repos*n_features],(n_repos,n_features))\n",
    "    X = np.reshape(args[n_repos*n_features:],(n_langs,n_features))\n",
    "    g = np.vectorize(lambda x: x*x)\n",
    "    return 0.5*np.sum(np.multiply(g(np.subtract(np.matmul(theta,np.transpose(X)),Y)),mask))+reg_param/2*np.sum(g(args))\n",
    "\n",
    "#gradient\n",
    "def gradient(args, Y=mat):\n",
    "    theta = np.reshape(args[:n_repos*n_features],(n_repos,n_features))\n",
    "    X = np.reshape(args[n_repos*n_features:],(n_langs,n_features))\n",
    "    X_grad = np.matmul(np.transpose(np.multiply(np.subtract(np.matmul(theta,np.transpose(X)),Y),mask)),theta)+reg_param*X\n",
    "    theta_grad = np.matmul(np.multiply(np.subtract(np.matmul(theta,np.transpose(X)),Y),mask),X)+reg_param*theta\n",
    "    return np.concatenate((np.reshape(theta_grad,-1),np.reshape(X_grad,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is performed using loss and gradient as defined above. This will iteratively improve matrices theta and X, so that their product more closely matches the target matrix Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "def train(Y=mat):\n",
    "    theta = np.random.rand(n_repos,n_features)\n",
    "    X = np.random.rand(n_langs,n_features)\n",
    "    args = np.concatenate((np.reshape(theta,-1),np.reshape(X,-1)))\n",
    "\n",
    "    args = op.fmin_cg(loss,args,gradient,maxiter=10000)\n",
    "\n",
    "    theta = np.reshape(args[:n_repos*n_features],(n_repos,n_features))\n",
    "    X = np.reshape(args[n_repos*n_features:],(n_langs,n_features))\n",
    "    \n",
    "    return theta,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can test the recommender system. Type in your favorite language (or a space-separated list of your favourite languages), and 5 new languages will be recommended for you to learn. The training process may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some languages: html css\n",
      "\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.001695\n",
      "         Iterations: 10000\n",
      "         Function evaluations: 15290\n",
      "         Gradient evaluations: 15290\n",
      "\n",
      "1: Haskell - 1.29583885423\n",
      "2: Clojure - 1.20321059029\n",
      "3: Scala - 1.13094744043\n",
      "4: Perl - 1.03885669958\n",
      "5: Erlang - 1.01627024842\n"
     ]
    }
   ],
   "source": [
    "string = raw_input(\"Enter some languages: \")\n",
    "print\n",
    "langs = string.split(' ')\n",
    "lc_names = {str(name).lower(): name_to_index[name] for name in name_to_index}\n",
    "\n",
    "test = np.zeros((1,len(names)))\n",
    "known = set()\n",
    "for lang in langs:\n",
    "    if lang.lower() in lc_names:\n",
    "        test[0][lc_names[lang.lower()]] = 1\n",
    "        known.add(lc_names[lang.lower()])\n",
    "                       \n",
    "mat = np.concatenate((mat,test[:,range(filter_size)]),0)\n",
    "mask,n_repos,n_langs = init_mask()\n",
    "theta,X = train(mat)\n",
    "mat = mat[:-1]\n",
    "    \n",
    "predictions = np.matmul(theta,np.transpose(X))[-1].tolist()\n",
    "predictions = sorted([(abs(j),i) for i,j in enumerate(predictions)],reverse=True)\n",
    "    \n",
    "print\n",
    "i = 0\n",
    "for val,name in predictions:\n",
    "    if name not in known:\n",
    "        print str(i+1)+': '+names[name]+' - '+str(val)\n",
    "        i+=1\n",
    "    if i>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you type in the list of languages, the recommender system adds that extra row to target matrix Y. Then, training is performed on your language preferences simultaneously as those of all the repositories in the sample. Finally, the trained matrices theta and X are multiplied, and the last row corresponds to the predicted ratings based on your preferences. The highest values are the languages recommended to you. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
